{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd945dbe-24c6-4fa6-a43e-35a5aefdcb6d",
   "metadata": {},
   "source": [
    "**本文处于开发中，尚不能通篇正常运行**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9593f23-5c3b-424e-b37b-47973fb5f13a",
   "metadata": {},
   "source": [
    "## 参考\n",
    "1. [Spark_SQL_cheatsheet_code.ipynb](https://github.com/ShowMeAI-Hub/awesome-AI-cheatsheets/blob/main/Spark/Spark_SQL_cheatsheet_code.ipynb)， 镜像在gitee [ShowMeAI-Hub--awesome-AI-cheatsheets.git](https://gitee.com/mirrr/ShowMeAI-Hub--awesome-AI-cheatsheets/tree/main/Spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ec7930",
   "metadata": {},
   "source": [
    "## 本jupyter环境启动\n",
    "\n",
    "```shell\n",
    "source /app/Miniconda3-py310_22.11.1-1/bin/activate\n",
    "cd /fridaAnlzAp/analyze_by_spark_GraphFrame/\n",
    "jupyter notebook &\n",
    "```\n",
    "\n",
    "\n",
    "本文地址， http://localhost:8888/notebooks/demo.ipynb  ,  \n",
    "在```jupyter notebook``` 的 web页面下 ```Help-->Show Keyboard Shortcuts```中找到 文章目录toc 的快捷键是 Ctrl+Shit+k "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6bb2800-6bb2-4dfe-b3fe-e2bdfa263753",
   "metadata": {},
   "source": [
    "## 基础软件环境\n",
    "\n",
    "#### jdk8\n",
    "https://cdn.azul.com/zulu/bin/zulu8.76.0.17-ca-jdk8.0.402-linux_x64.tar.gz\n",
    "\n",
    "\n",
    "#### spark 3.5.0 \n",
    "\n",
    "[spark-3.5.0-bin-hadoop3.tgz](https://archive.apache.org/dist/spark/spark-3.5.0/spark-3.5.0-bin-hadoop3.tgz)发布时间为  ``` 2023-09-09```\n",
    "\n",
    "https://archive.apache.org/dist/spark/spark-3.5.0\n",
    "\n",
    "####  放弃 graphframes 0.8.3-spark3.5-s_2.13,因为 其  最高只能用于 spark 2.4+\n",
    "https://spark-packages.org/package/graphframes/graphframes  , 其中 ```0.8.3-spark3.5-s_2.13```发布时间为 ```2023-10-07 ``` ，与 上面  spark 发布时间 较为接近\n",
    "\n",
    "\n",
    "graphframes提供 ```from graphframes import GraphFrame```\n",
    "\n",
    "\n",
    "```This project is compatible with Spark 2.4+. ```， 这就说明 graphframes 不可能用于 spark 3.5.0 ， \n",
    "来自  [graphframes  spark-version-compatibility](https://github.com/graphframes/graphframes?tab=readme-ov-file#spark-version-compatibility)\n",
    "\n",
    "####  scala\n",
    "\n",
    "根据 ```Upgrade Scala to 2.12.18 SPARK-43832```,  https://spark.apache.org/releases/spark-release-3-5-0.html\n",
    "\n",
    "可知 spark 3.5.0 应该用  scala 2.12.18    ,  https://www.scala-lang.org/download/2.12.18.html  ,  https://downloads.lightbend.com/scala/2.12.18/scala-2.12.18.tgz\n",
    "\n",
    "[SPARK-43832](https://issues.apache.org/jira/browse/SPARK-43832)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba2ceae",
   "metadata": {},
   "source": [
    "####  基础软件材料\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0abacaf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4a3b0cfc251827d78ff468db7016f2ee  /app/pack/spark-3.5.0-bin-hadoop3.tgz\n",
      "3d8073a1e7bc71a0c53bbbbad590dad2  /app/pack/zulu8.76.0.17-ca-jdk8.0.402-linux_x64.tar.gz\n",
      "ad62b61e56da41e4a0c1a64d33c4f187  /app/pack/scala-2.12.18.tgz\n",
      "-rwxrwxrwx 1 z z 263M  3月 24 16:57 /fridaAnlzAp/frida_js/frida-trace-out-RunBuszJs-1711270533.log\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "md5sum \\\n",
    "/app/pack/spark-3.5.0-bin-hadoop3.tgz \\\n",
    "/app/pack/zulu8.76.0.17-ca-jdk8.0.402-linux_x64.tar.gz \\\n",
    "/app/pack/scala-2.12.18.tgz\n",
    "\n",
    "ls -lh /fridaAnlzAp/frida_js/*RunBuszJs*.log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b842758",
   "metadata": {},
   "source": [
    "#### 基础软件状况\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b4438b3-79fd-4178-a338-7111e5d81050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024年 03月 26日 星期二 15:36:20 CST\n",
      "/fridaAnlzAp/analyze_by_spark_GraphFrame\n",
      "/app/Miniconda3-py310_22.11.1-1/bin/python\n",
      "/app/Miniconda3-py310_22.11.1-1/bin/pip\n",
      "/app/zulu8.76.0.17-ca-jdk8.0.402-linux_x64: directory\n",
      "/app/spark-3.5.0-bin-hadoop3:               directory\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "date && pwd\n",
    "\n",
    "which python\n",
    "which pip\n",
    "\n",
    "file \\\n",
    "/app/zulu8.76.0.17-ca-jdk8.0.402-linux_x64 \\\n",
    "/app/spark-3.5.0-bin-hadoop3 \\\n",
    "/app/scala-2.12.18/\n",
    "\n",
    "\n",
    "#tar -zxf /app/pack/spark-3.5.0-bin-hadoop3.tgz -C /app/\n",
    "#/app/spark-3.5.0-bin-hadoop3/\n",
    "\n",
    "#tar -zxf  /app/pack/scala-2.12.18.tgz  -C /app/\n",
    "#/app/scala-2.12.18/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311339f2",
   "metadata": {},
   "source": [
    "## chrome ipynb 阅读器\n",
    "\n",
    "chrome ipynb 阅读器， https://github.com/imvladikon/jupyter-notebook-viewer?tab=readme-ov-file\n",
    "\n",
    "https://chrome.google.com/webstore/detail/jupyter-notebook-viewer/ocabfdicbcamoonfhalkdojedklfcjmf?hl=en\n",
    "\n",
    "\n",
    "\n",
    "本文地址， http://giteaz:3000/frida_analyze_app_src/analyze_by_spark_GraphFrame/raw/branch/main/demo.ipynb ,  chrome浏览器打开文本 Ctrl+F5 强制刷新 可获得到 更新后 内容"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a78174",
   "metadata": {},
   "source": [
    "## java、spark环境指定"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f83abed",
   "metadata": {},
   "source": [
    "### 指定 JAVA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca528c7b-c1b5-4e82-a911-6d9fd06540d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"JAVA_HOME\"]=\"/app/zulu8.76.0.17-ca-jdk8.0.402-linux_x64\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05036ca3",
   "metadata": {},
   "source": [
    "### 指定   SPARK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb04f71a",
   "metadata": {},
   "source": [
    "只要指定了环境变量SPARK_HOME即可，以下两个方式等效"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3df4359",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"SPARK_HOME\"]=\"/app/spark-3.5.0-bin-hadoop3\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01c1669-11f3-4488-a8cd-5a486fdb71ea",
   "metadata": {},
   "source": [
    "### 指定   scala"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0cfc5b9c-0f6c-4d9b-8b4f-d480348ac19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"SCALA_HOME\"]=\"/app/scala-2.12.18/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec97fc30-5b3d-4731-b53e-9a6b4fe1993b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/app/Miniconda3-py310_22.11.1-1/bin:/app/Miniconda3-py37_4.12.0/condabin:/app/bin:/app/nvm/versions/node/v18.19.1/bin:/home/z/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/app/bash-simplify/bash-complete-gen-from-help/bin:/app/cmd-wrap/tool_bin:/app/zulu8.76.0.17-ca-jdk8.0.402-linux_x64/bin:/app/spark-3.5.0-bin-hadoop3/bin:/app/scala-2.12.18//bin\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"PATH\"]=f'{os.environ[\"PATH\"]}:{os.environ[\"JAVA_HOME\"]}/bin:{os.environ[\"SPARK_HOME\"]}/bin:{os.environ[\"SCALA_HOME\"]}/bin'\n",
    "print(os.environ[\"PATH\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755ded69",
   "metadata": {},
   "source": [
    "### 本进程中的 环境变量 *_HOME "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e49ba830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JAVA_HOME=/app/zulu8.76.0.17-ca-jdk8.0.402-linux_x64\n",
      "SCALA_HOME=/app/scala-2.12.18/\n",
      "SPARK_HOME=/app/spark-3.5.0-bin-hadoop3\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "env | grep _HOME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517959be",
   "metadata": {},
   "source": [
    "##  基本python依赖\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2565c169",
   "metadata": {},
   "source": [
    "###  spark-3.5.0 自带 pyspark3.5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0270754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__version__: str = '3.5.0'\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "cat $SPARK_HOME/python/pyspark/version.py \n",
    "# cat  /app/spark-3.5.0-bin-hadoop3/python/pyspark/version.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4936f0fc-38cd-44e6-8e7d-b7a6dcfba9fa",
   "metadata": {},
   "source": [
    "\n",
    "#### spark自带的pyshark缺少依赖\n",
    "\n",
    "假如试图使用 spark自带的pyshark , 但却没有携带相应依赖 比如 py4j\n",
    "____\n",
    "\n",
    "启动jupyter前，设置PYTHONPATH 试图使用spark自带的pyshark\n",
    "```shell\n",
    "# /app/spark-3.5.0-bin-hadoop3/python/pyshark\n",
    "source /app/Miniconda3-py37_4.12.0/bin/activate\n",
    "export PYTHONPATH=/app/spark-3.5.0-bin-hadoop3/python/\n",
    "jupyter notebook\n",
    "```\n",
    "\n",
    "在jupyter页面中，试图使用pyshark, 报错是 ```No module named 'py4j'```, 可见 找到了pyshark 但没有其依赖的py4j\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "#ModuleNotFoundError: No module named 'py4j'\n",
    "```\n",
    "\n",
    "因此, 不如对 ```/app/Miniconda3-py310```从新安装pyshark 3.5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "314e8904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: findspark==2.0.1 in /app/Miniconda3-py37_4.12.0/lib/python3.7/site-packages (2.0.1)\n",
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: pandas==1.3.5 in /app/Miniconda3-py37_4.12.0/lib/python3.7/site-packages (1.3.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /app/Miniconda3-py37_4.12.0/lib/python3.7/site-packages (from pandas==1.3.5) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2017.3 in /app/Miniconda3-py37_4.12.0/lib/python3.7/site-packages (from pandas==1.3.5) (2024.1)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /app/Miniconda3-py37_4.12.0/lib/python3.7/site-packages (from pandas==1.3.5) (1.21.6)\n",
      "Requirement already satisfied: six>=1.5 in /app/Miniconda3-py37_4.12.0/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas==1.3.5) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "#只在 'import findspark' 用到\n",
    "pip install findspark==2.0.1\n",
    "\n",
    "#只在 df1.toPandas() 用到\n",
    "pip install pandas==1.3.5\n",
    "\n",
    "#先不指定版本号，直接安装，人工观看版本号后，将版本号写到上面的\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e83fc93",
   "metadata": {},
   "source": [
    "### 对 Miniconda3-py310 安装 pyspark 3.5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e59e535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://mirrors.ustc.edu.cn/pypi/web/simple\n",
      "Requirement already satisfied: pyspark==3.5.0 in /app/Miniconda3-py310_22.11.1-1/lib/python3.10/site-packages (3.5.0)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /app/Miniconda3-py310_22.11.1-1/lib/python3.10/site-packages (from pyspark==3.5.0) (0.10.9.7)\n",
      "Name: pyspark\n",
      "Version: 3.5.0\n",
      "Summary: Apache Spark Python API\n",
      "Home-page: https://github.com/apache/spark/tree/master/python\n",
      "Author: Spark Developers\n",
      "Author-email: dev@spark.apache.org\n",
      "License: http://www.apache.org/licenses/LICENSE-2.0\n",
      "Location: /app/Miniconda3-py310_22.11.1-1/lib/python3.10/site-packages\n",
      "Requires: py4j\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "pip install  -i https://mirrors.ustc.edu.cn/pypi/web/simple  pyspark==3.5.0\n",
    "#中科大pypi比清华pypi快\n",
    "\n",
    "pip show pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea730fc",
   "metadata": {},
   "source": [
    "## graphframes安装\n",
    "\n",
    "根据，``` 2023/09/27``` ,   https://github.com/graphframes/graphframes/releases/tag/v0.8.3 ， 的 说明\n",
    "\n",
    "```Support Spark 3.3 / Scala 2.12 , Spark 3.4 / Scala 2.12 and Scala 2.13, Spark 3.5 / Scala 2.12 and Scala 2.13```\n",
    "\n",
    "当前是spark 3.5.0 , 应该安装 graphframes v0.8.3 \n",
    "\n",
    "\n",
    "注意 graphframes 在pypi上 有疑似废弃的graphframes和感觉正常的graphframes-latest\n",
    "\n",
    "\n",
    "用 https://pypi.org/project/graphframes-latest/#history\n",
    "\n",
    "不要用 https://pypi.org/project/graphframes/#history\n",
    "\n",
    "\n",
    "### 安装 graphframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c94910b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Collecting graphframes-latest==0.8.3\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/9d/6b/053301dc7a0f2a023ff1066a2016bee2826000f445a00a9c07fc7db5b61e/graphframes_latest-0.8.3.tar.gz (15 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting nose==1.3.7\n",
      "  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/15/d8/dd071918c040f50fa1cf80da16423af51ff8ce4a0f2399b7bf8de45ac3d9/nose-1.3.7-py3-none-any.whl (154 kB)\n",
      "Collecting numpy>=1.7\n",
      "  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/4b/d7/ecf66c1cd12dc28b4040b15ab4d17b773b87fa9d29ca16125de01adb36cd/numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
      "Building wheels for collected packages: graphframes-latest\n",
      "  Building wheel for graphframes-latest (setup.py): started\n",
      "  Building wheel for graphframes-latest (setup.py): finished with status 'done'\n",
      "  Created wheel for graphframes-latest: filename=graphframes_latest-0.8.3-py3-none-any.whl size=21233 sha256=0798885086eb74517aa93cdef8432534271f3e95ce88b1b7c262ed3308ff9aa4\n",
      "  Stored in directory: /home/z/.cache/pip/wheels/29/05/69/17152b8b968d40353c3a163d1f88d3f27731e91ec1094d0f89\n",
      "Successfully built graphframes-latest\n",
      "Installing collected packages: nose, numpy, graphframes-latest\n",
      "Successfully installed graphframes-latest-0.8.3 nose-1.3.7 numpy-1.26.4\n",
      "Name: graphframes-latest\n",
      "Version: 0.8.3\n",
      "Summary: GraphFrames: DataFrame-based Graphs\n",
      "Home-page: https://github.com/graphframes/graphframes\n",
      "Author: \n",
      "Author-email: \n",
      "License: MIT\n",
      "Location: /app/Miniconda3-py310_22.11.1-1/lib/python3.10/site-packages\n",
      "Requires: nose, numpy\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "pip install graphframes-latest==0.8.3\n",
    "pip show graphframes-latest\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271457c2",
   "metadata": {},
   "source": [
    "park-2.4.8 自带 pyspark2.4.8， \n",
    "而 这里 又 对 Miniconda3-py37 安装 pyspark2.4.8 ，\n",
    "显然有一处是多余的，\n",
    "这里目前没搞清楚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9740eb4a-288d-43c8-9198-6e8654148e40",
   "metadata": {},
   "source": [
    "####  存放 graphframes-0.8.3-spark3.5-s_2.13.jar 的 目录\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a1e7564",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/26 17:33:59 WARN Utils: Your hostname, mchr resolves to a loopback address: 127.0.1.1; using 10.0.4.23 instead (on interface wlo1)\n",
      "24/03/26 17:33:59 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/03/26 17:33:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "       .builder \\\n",
    "       .appName(\"analyze_by_spark_GraphFrame\") \\\n",
    "       .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "       .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f9a6603-7752-4306-9f16-971bb31789d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.some.config.option', 'some-value'),\n",
       " ('spark.driver.port', '32941'),\n",
       " ('spark.app.submitTime', '1711445639596'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.driver.extraJavaOptions',\n",
       "  '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false'),\n",
       " ('spark.app.id', 'local-1711445640369'),\n",
       " ('spark.app.startTime', '1711445639758'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.sql.warehouse.dir',\n",
       "  'file:/fridaAnlzAp/analyze_by_spark_GraphFrame/spark-warehouse'),\n",
       " ('spark.driver.host', 'github.local'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.master', 'local[*]'),\n",
       " ('spark.submit.pyFiles', ''),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.app.name', 'analyze_by_spark_GraphFrame'),\n",
       " ('spark.ui.showConsoleProgress', 'true'),\n",
       " ('spark.executor.extraJavaOptions',\n",
       "  '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b12a8b-0751-48a1-a141-ee475e954d4d",
   "metadata": {},
   "source": [
    "SparkSession.builder启动了一个spark进程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b5bf75ee-7294-4b38-8cfc-b76b34f6933e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/app/zulu8.76.0.17-ca-jdk8.0.402-linux_x64/bin/java -cp /app/spark-3.5.0-bin-hadoop3//conf/:/app/spark-3.5.0-bin-hadoop3/jars/* -Xmx1g -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false org.apache.spark.deploy.SparkSubmit --conf spark.app.name=example pyspark-shell "
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "  \n",
    "#SparkSession.builder启动的spark进程命令行如下\n",
    "tr '\\0' ' '  <   /proc/`pidof java`/cmdline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670d3bf0-72c5-432a-ab2e-3ca39329d868",
   "metadata": {},
   "source": [
    "**从上面spark进程的命令行参数中的```-cp```明显知道，```graphframes-0.8.3-spark3.5-s_2.13.jar```应该放到目录 ```/app/spark-3.5.0-bin-hadoop3/jars/```**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca20fd8",
   "metadata": {},
   "source": [
    "### 安装 graphframes-0.8.3-spark3.5-s_2.13.jar\n",
    "\n",
    "知道了 存放该 jar的目录， 安装 只是 复制到该目录而已"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f3c5a90-8075-4979-923b-1aa5fd04f3d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--2024-03-26 17:01:58--  https://repos.spark-packages.org/graphframes/graphframes/0.8.3-spark3.5-s_2.13/graphframes-0.8.3-spark3.5-s_2.13.jar\n",
      "正在解析主机 repos.spark-packages.org (repos.spark-packages.org)... 54.230.61.22, 54.230.61.9, 54.230.61.26, ...\n",
      "正在连接 repos.spark-packages.org (repos.spark-packages.org)|54.230.61.22|:443... 已连接。\n",
      "已发出 HTTP 请求，正在等待回应... 200 OK\n",
      "长度： 251438 (246K) [binary/octet-stream]\n",
      "正在保存至: ‘/app/spark-3.5.0-bin-hadoop3/jars/graphframes-0.8.3-spark3.5-s_2.13.jar’\n",
      "\n",
      "     0K .......... .......... .......... .......... .......... 20%  154K 1s\n",
      "    50K .......... .......... .......... .......... .......... 40%  173K 1s\n",
      "   100K .......... .......... .......... .......... .......... 61%  511K 0s\n",
      "   150K .......... .......... .......... .......... .......... 81%  464K 0s\n",
      "   200K .......... .......... .......... .......... .....     100%  430K=0.9s\n",
      "\n",
      "2024-03-26 17:02:00 (265 KB/s) - 已保存 ‘/app/spark-3.5.0-bin-hadoop3/jars/graphframes-0.8.3-spark3.5-s_2.13.jar’ [251438/251438])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "graphframesJarUrl=https://repos.spark-packages.org/graphframes/graphframes/0.8.3-spark3.5-s_2.13/graphframes-0.8.3-spark3.5-s_2.13.jar\n",
    "dstJarF=/app/spark-3.5.0-bin-hadoop3/jars/graphframes-0.8.3-spark3.5-s_2.13.jar\n",
    "wget --output-document=$dstJarF $graphframesJarUrl\n",
    "\n",
    "# md5sum graphframes-0.8.3-spark3.5-s_2.13.jar == 41096922bf65505c2b89865350123e11"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd29318f",
   "metadata": {},
   "source": [
    "##  graphframes例子（pySpark图结构例子）\n",
    "\n",
    "\n",
    "看起来graphframes不支持多个边字段?\n",
    "\n",
    "所以 可能要 用基于scala的GraphX,   然后  可以将 GraphX 转回 GraphFrame ？\n",
    "\n",
    "还是说 直接 就用 scala的GraphX?  \n",
    "____\n",
    "\n",
    " GraphX 转回 GraphFrame,  https://graphframes.github.io/graphframes/docs/_site/user-guide.html#example-conversions\n",
    " \n",
    " \n",
    " graphframes是独立于spark的，只有900多个星星， https://github.com/graphframes/graphframes\n",
    " \n",
    " 而 pyspark、graphX是spark项目的一部分， 38k个星星， https://github.com/apache/spark/tree/master/python/pyspark  ，  https://github.com/apache/spark/tree/master/graphx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c939a83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/26 17:34:12 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o73.createGraph.\n: java.lang.NoClassDefFoundError: scala/collection/ArrayOps$\n\tat org.graphframes.GraphFrame$.apply(GraphFrame.scala:676)\n\tat org.graphframes.GraphFramePythonAPI.createGraph(GraphFramePythonAPI.scala:10)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.lang.ClassNotFoundException: scala.collection.ArrayOps$\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:387)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:419)\n\tat sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:352)\n\t... 14 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 26\u001b[0m\n\u001b[1;32m     18\u001b[0m edges \u001b[38;5;241m=\u001b[39m _sparkSession\u001b[38;5;241m.\u001b[39mcreateDataFrame([\n\u001b[1;32m     19\u001b[0m     (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfriend\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     20\u001b[0m     (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfollow\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     21\u001b[0m     (\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfriend\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     22\u001b[0m     (\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfollow\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     23\u001b[0m ], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msrc\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdst\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrelationship\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m#看起来graphframes不支持多个边字段?\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m graph \u001b[38;5;241m=\u001b[39m \u001b[43mGraphFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvertices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medges\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m graph\u001b[38;5;241m.\u001b[39mvertices\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m     29\u001b[0m graph\u001b[38;5;241m.\u001b[39medges\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m/app/Miniconda3-py310_22.11.1-1/lib/python3.10/site-packages/graphframes/graphframe.py:89\u001b[0m, in \u001b[0;36mGraphFrame.__init__\u001b[0;34m(self, v, e)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mDST \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m e\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     86\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDestination vertex ID column \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m missing from edge DataFrame, which has columns: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     87\u001b[0m         \u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mDST, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(e\u001b[38;5;241m.\u001b[39mcolumns)))\n\u001b[0;32m---> 89\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm_graph \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm_gf_api\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreateGraph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43me\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/app/Miniconda3-py310_22.11.1-1/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/app/Miniconda3-py310_22.11.1-1/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/app/Miniconda3-py310_22.11.1-1/lib/python3.10/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o73.createGraph.\n: java.lang.NoClassDefFoundError: scala/collection/ArrayOps$\n\tat org.graphframes.GraphFrame$.apply(GraphFrame.scala:676)\n\tat org.graphframes.GraphFramePythonAPI.createGraph(GraphFramePythonAPI.scala:10)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.lang.ClassNotFoundException: scala.collection.ArrayOps$\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:387)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:419)\n\tat sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:352)\n\t... 14 more\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"JAVA_HOME\"]=\"/app/zulu8.76.0.17-ca-jdk8.0.402-linux_x64\"\n",
    "os.environ[\"SPARK_HOME\"]=\"/app/spark-3.5.0-bin-hadoop3/\"\n",
    "os.environ[\"SCALA_HOME\"]=\"/app/scala-2.12.18/\"\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from graphframes import GraphFrame\n",
    "\n",
    "_sparkSession = SparkSession.builder.appName(\"example\").getOrCreate()\n",
    "\n",
    "vertices = _sparkSession.createDataFrame([\n",
    "    (0, \"Alice\", 34),\n",
    "    (1, \"Bob\", 36),\n",
    "    (2, \"Charlie\", 30),\n",
    "    (3, \"David\", 29)\n",
    "], [\"id\", \"name\", \"age\"])\n",
    "\n",
    "edges = _sparkSession.createDataFrame([\n",
    "    (0, 1, \"friend\"),\n",
    "    (1, 2, \"follow\"),\n",
    "    (2, 0, \"friend\"),\n",
    "    (3, 0, \"follow\")\n",
    "], [\"src\", \"dst\", \"relationship\"])\n",
    "#看起来graphframes不支持多个边字段?\n",
    "\n",
    "graph = GraphFrame(vertices, edges)\n",
    "\n",
    "graph.vertices.show()\n",
    "graph.edges.show()\n",
    "\n",
    "print(\"Alice的邻居：\")\n",
    "graph.find(\"(a)-[]->(b); (b)-[]->(c)\").filter(\"a.name = 'Alice'\").select(\"c.name\").show()\n",
    "\n",
    "_sparkSession.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c18e58-29a8-420b-bfd3-65e40b79de71",
   "metadata": {},
   "source": [
    "##  以下没有运行"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cabaf60",
   "metadata": {},
   "source": [
    "## 正文（pySpark用法举例） \n",
    "\n",
    "\n",
    "#### SparkSession \n",
    "\n",
    "用于创建数据帧，将数据帧注册为表，执行 SQL 查询，缓存表及读取 Parquet 文件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5b04b4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2fd321c",
   "metadata": {},
   "source": [
    "###  从数据文件创建pyspark.DataFrame\n",
    "\n",
    "【术语】pyspark.sql.dataframe.DataFrame == pyspark.DataFrame\n",
    "\n",
    "#### 从json文件创建 pyspark.DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ccedde18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rwxrwxrwx 1 z z 263M  3月 24 16:57 /fridaAnlzAp/frida_js/frida-trace-out-RunBuszJs-1711270533.log\n",
      "1045003 /fridaAnlzAp/frida_js/frida-trace-out-RunBuszJs-1711270533.log\n",
      "{\"curThreadId\":392463,\"direct\":1,\"fnAdr\":\"0x55555556b4fe\",\"fnCallId\":1,\"fnSym\":{\"address\":\"0x55555556b4fe\",\"name\":\"_ZNSt12_Vector_baseISt10shared_ptrIN5torch3jit6script4TreeEESaIS5_EE12_Vector_implC1Ev\",\"moduleName\":\"simple_nn.elf\",\"fileName\":\"\",\"lineNumber\":0,\"column\":0}}\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "FP=/fridaAnlzAp/frida_js/frida-trace-out-RunBuszJs-1711270533.log\n",
    "ls -lh $FP\n",
    "wc -l $FP\n",
    "head -n 1 $FP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c63ae5ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df0 = spark.read.json(\"/fridaAnlzAp/frida_js/frida-trace-out-RunBuszJs-1711270533.log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f192d73c",
   "metadata": {},
   "source": [
    "##### pyspark.sql.dataframe.DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4519092d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4763aec2",
   "metadata": {},
   "source": [
    "### pyspark.DataFrame列概览\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923dab8b",
   "metadata": {},
   "source": [
    "####  列名"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "12b2959f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['curThreadId', 'direct', 'fnAdr', 'fnCallId', 'fnSym']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df0.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eded3e0",
   "metadata": {},
   "source": [
    "#### 列名、列类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4faf525d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('curThreadId', 'bigint'),\n",
       " ('direct', 'bigint'),\n",
       " ('fnAdr', 'string'),\n",
       " ('fnCallId', 'bigint'),\n",
       " ('fnSym',\n",
       "  'struct<address:string,column:bigint,fileName:string,lineNumber:bigint,moduleName:string,name:string>')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df0.dtypes "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5461341",
   "metadata": {},
   "source": [
    "#### 列名、列类型、层级结构"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602fe97b",
   "metadata": {},
   "source": [
    "##### 人类可读"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "edbda13c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- curThreadId: long (nullable = true)\n",
      " |-- direct: long (nullable = true)\n",
      " |-- fnAdr: string (nullable = true)\n",
      " |-- fnCallId: long (nullable = true)\n",
      " |-- fnSym: struct (nullable = true)\n",
      " |    |-- address: string (nullable = true)\n",
      " |    |-- column: long (nullable = true)\n",
      " |    |-- fileName: string (nullable = true)\n",
      " |    |-- lineNumber: long (nullable = true)\n",
      " |    |-- moduleName: string (nullable = true)\n",
      " |    |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df0.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ec1704",
   "metadata": {},
   "source": [
    "##### 机器使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f4558c60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(curThreadId,LongType,true),StructField(direct,LongType,true),StructField(fnAdr,StringType,true),StructField(fnCallId,LongType,true),StructField(fnSym,StructType(List(StructField(address,StringType,true),StructField(column,LongType,true),StructField(fileName,StringType,true),StructField(lineNumber,LongType,true),StructField(moduleName,StringType,true),StructField(name,StringType,true))),true)))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df0.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a535018",
   "metadata": {},
   "source": [
    "### pyspark.DataFrame行概览\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08284043",
   "metadata": {},
   "source": [
    "#### 描述\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "90362d8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:>                                                        (0 + 16) / 16]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+------------------+--------------+------------------+\n",
      "|summary|curThreadId|            direct|         fnAdr|          fnCallId|\n",
      "+-------+-----------+------------------+--------------+------------------+\n",
      "|  count|    1045003|           1045003|       1045003|           1045003|\n",
      "|   mean|   392463.0|1.4999995215324742|          null|261251.25001746407|\n",
      "| stddev|        0.0|0.5000002392337048|          null|150833.26303729884|\n",
      "|    min|     392463|                 1|0x55555556658f|                 1|\n",
      "|    max|     392463|                 2|0x7ffff7807ad4|            522502|\n",
      "+-------+-----------+------------------+--------------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df0.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ae9352",
   "metadata": {},
   "source": [
    "####  显示前n行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "23e2a3b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+--------------+--------+--------------------+\n",
      "|curThreadId|direct|         fnAdr|fnCallId|               fnSym|\n",
      "+-----------+------+--------------+--------+--------------------+\n",
      "|     392463|     1|0x55555556b4fe|       1|[0x55555556b4fe, ...|\n",
      "|     392463|     1|0x55555556b752|       2|[0x55555556b752, ...|\n",
      "|     392463|     1|0x55555556cc0c|       3|[0x55555556cc0c, ...|\n",
      "|     392463|     2|0x55555556cc0c|       3|[0x55555556cc0c, ...|\n",
      "|     392463|     2|0x55555556b752|       2|[0x55555556b752, ...|\n",
      "|     392463|     1|0x55555556ca72|       4|[0x55555556ca72, ...|\n",
      "|     392463|     2|0x55555556ca72|       4|[0x55555556ca72, ...|\n",
      "|     392463|     2|0x55555556b4fe|       1|[0x55555556b4fe, ...|\n",
      "|     392463|     1|0x7ffff770930c|       5|[0x7ffff770930c, ...|\n",
      "|     392463|     1|0x7ffff770ef00|       6|[0x7ffff770ef00, ...|\n",
      "|     392463|     1|0x7ffff771130c|       7|[0x7ffff771130c, ...|\n",
      "|     392463|     1|0x7ffff6d19966|       8|[0x7ffff6d19966, ...|\n",
      "|     392463|     2|0x7ffff6d19966|       8|[0x7ffff6d19966, ...|\n",
      "|     392463|     2|0x7ffff771130c|       7|[0x7ffff771130c, ...|\n",
      "|     392463|     2|0x7ffff770ef00|       6|[0x7ffff770ef00, ...|\n",
      "|     392463|     1|0x7ffff4bf84ea|       9|[0x7ffff4bf84ea, ...|\n",
      "|     392463|     1|0x7ffff4bf9326|      10|[0x7ffff4bf9326, ...|\n",
      "|     392463|     2|0x7ffff4bf9326|      10|[0x7ffff4bf9326, ...|\n",
      "|     392463|     1|0x7ffff74ba1c1|      11|[0x7ffff74ba1c1, ...|\n",
      "|     392463|     1|0x7ffff74be6f8|      12|[0x7ffff74be6f8, ...|\n",
      "+-----------+------+--------------+--------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df0.show() #默认n=20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26aa5bc",
   "metadata": {},
   "source": [
    "####   行数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "52a9bad4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1045003"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df0.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676f5c81",
   "metadata": {},
   "source": [
    "####   不重复行数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3f0089d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1045003"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df0.distinct().count() #返回 df 中不重复的行数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57946880",
   "metadata": {},
   "source": [
    "####   explain\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "70b81a38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) FileScan json [curThreadId#107L,direct#108L,fnAdr#109,fnCallId#110L,fnSym#111] Batched: false, Format: JSON, Location: InMemoryFileIndex[file:/fridaAnlzAp/frida_js/frida-trace-out-RunBuszJs-1711270533.log], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<curThreadId:bigint,direct:bigint,fnAdr:string,fnCallId:bigint,fnSym:struct<address:string,...\n"
     ]
    }
   ],
   "source": [
    "df0.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943ea6f1",
   "metadata": {},
   "source": [
    "### pyspark.DataFrame列编辑"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61cb0acb",
   "metadata": {},
   "source": [
    "####  复杂字段展平：从列fnSym的内部字段新增列\n",
    "\n",
    "\n",
    "- fnSym_address <--fnSym.address\n",
    "- fnSym_column <--fnSym.column\n",
    "- fnSym_fileName <--fnSym.fileName\n",
    "- fnSym_lineNumber <--fnSym.lineNumber\n",
    "- fnSym_moduleName <--fnSym.moduleName\n",
    "- fnSym_name <--fnSym.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "247c3f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df0 \\\n",
    ".withColumn('fnSym_address',df0.fnSym.address) \\\n",
    ".withColumn('fnSym_column',df0.fnSym.column) \\\n",
    ".withColumn('fnSym_fileName',regexp_replace(df0.fnSym.fileName, '/home/z/torch-repo/pytorch/', './')) \\\n",
    ".withColumn('fnSym_lineNumber',df0.fnSym.lineNumber) \\\n",
    ".withColumn('fnSym_moduleName', df0.fnSym.moduleName) \\\n",
    ".withColumn('fnSym_name', df0.fnSym[\"name\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395ebf5a",
   "metadata": {},
   "source": [
    "#### describe竖直显示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3d8c0f29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:==========>                                              (3 + 13) / 16]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0--------------------------------\n",
      " summary          | count                \n",
      " curThreadId      | 1045003              \n",
      " direct           | 1045003              \n",
      " fnAdr            | 1045003              \n",
      " fnCallId         | 1045003              \n",
      " fnSym_address    | 1045003              \n",
      " fnSym_column     | 1045003              \n",
      " fnSym_fileName   | 1045003              \n",
      " fnSym_lineNumber | 1045003              \n",
      " fnSym_moduleName | 1045003              \n",
      " fnSym_name       | 1045003              \n",
      "-RECORD 1--------------------------------\n",
      " summary          | mean                 \n",
      " curThreadId      | 392463.0             \n",
      " direct           | 1.4999995215324742   \n",
      " fnAdr            | null                 \n",
      " fnCallId         | 261251.25001746407   \n",
      " fnSym_address    | null                 \n",
      " fnSym_column     | 7.957558016579856    \n",
      " fnSym_fileName   | null                 \n",
      " fnSym_lineNumber | 347.8104349939665    \n",
      " fnSym_moduleName | null                 \n",
      " fnSym_name       | null                 \n",
      "-RECORD 2--------------------------------\n",
      " summary          | stddev               \n",
      " curThreadId      | 0.0                  \n",
      " direct           | 0.5000002392337048   \n",
      " fnAdr            | null                 \n",
      " fnCallId         | 150833.26303729884   \n",
      " fnSym_address    | null                 \n",
      " fnSym_column     | 14.575796438734118   \n",
      " fnSym_fileName   | null                 \n",
      " fnSym_lineNumber | 501.64473275374263   \n",
      " fnSym_moduleName | null                 \n",
      " fnSym_name       | null                 \n",
      "-RECORD 3--------------------------------\n",
      " summary          | min                  \n",
      " curThreadId      | 392463               \n",
      " direct           | 1                    \n",
      " fnAdr            | 0x55555556658f       \n",
      " fnCallId         | 1                    \n",
      " fnSym_address    | 0x55555556658f       \n",
      " fnSym_column     | 0                    \n",
      " fnSym_fileName   |                      \n",
      " fnSym_lineNumber | 0                    \n",
      " fnSym_moduleName | libc10.so            \n",
      " fnSym_name       | CPUTensorId          \n",
      "-RECORD 4--------------------------------\n",
      " summary          | max                  \n",
      " curThreadId      | 392463               \n",
      " direct           | 2                    \n",
      " fnAdr            | 0x7ffff7807ad4       \n",
      " fnCallId         | 522502               \n",
      " fnSym_address    | 0x7ffff7807ad4       \n",
      " fnSym_column     | 131                  \n",
      " fnSym_fileName   | /usr/include/c++/... \n",
      " fnSym_lineNumber | 11854                \n",
      " fnSym_moduleName | simple_nn.elf        \n",
      " fnSym_name       | view                 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df1.describe().show(vertical=True)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3fd35ab",
   "metadata": {},
   "source": [
    "#### 前n行竖直显示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "51f97479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0--------------------------------\n",
      " curThreadId      | 392463               \n",
      " direct           | 1                    \n",
      " fnAdr            | 0x55555556b4fe       \n",
      " fnCallId         | 1                    \n",
      " fnSym            | [0x55555556b4fe, ... \n",
      " fnSym_address    | 0x55555556b4fe       \n",
      " fnSym_column     | 0                    \n",
      " fnSym_fileName   |                      \n",
      " fnSym_lineNumber | 0                    \n",
      " fnSym_moduleName | simple_nn.elf        \n",
      " fnSym_name       | _ZNSt12_Vector_ba... \n",
      "-RECORD 1--------------------------------\n",
      " curThreadId      | 392463               \n",
      " direct           | 1                    \n",
      " fnAdr            | 0x55555556b752       \n",
      " fnCallId         | 2                    \n",
      " fnSym            | [0x55555556b752, ... \n",
      " fnSym_address    | 0x55555556b752       \n",
      " fnSym_column     | 0                    \n",
      " fnSym_fileName   |                      \n",
      " fnSym_lineNumber | 0                    \n",
      " fnSym_moduleName | simple_nn.elf        \n",
      " fnSym_name       | _ZNSaISt10shared_... \n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show(vertical=True,n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7e2784",
   "metadata": {},
   "source": [
    "### pyspark.DataFrame查询\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491d774b",
   "metadata": {},
   "source": [
    "#### functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6ccdd3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45cdee66",
   "metadata": {},
   "source": [
    "#### select多列"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36bac883",
   "metadata": {},
   "source": [
    "#####  select的结果类型依然是pyspark.sql.dataframe.DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7ec9fe00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_sel_df=df1.select(\"fnSym_name\",\"fnSym_address\")\n",
    "type(_sel_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8556c7e",
   "metadata": {},
   "source": [
    "#####  人类可读"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3fc5b4d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------+\n",
      "|          fnSym_name| fnSym_address|\n",
      "+--------------------+--------------+\n",
      "|_ZNSt12_Vector_ba...|0x55555556b4fe|\n",
      "|_ZNSaISt10shared_...|0x55555556b752|\n",
      "|_ZN9__gnu_cxx13ne...|0x55555556cc0c|\n",
      "+--------------------+--------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_sel_df.show(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7177b99",
   "metadata": {},
   "source": [
    "#####  机器使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f396bc8f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(fnSym_name='_ZNSt12_Vector_baseISt10shared_ptrIN5torch3jit6script4TreeEESaIS5_EE12_Vector_implC1Ev', fnSym_address='0x55555556b4fe'),\n",
       " Row(fnSym_name='_ZNSaISt10shared_ptrIN5torch3jit6script4TreeEEEC1Ev', fnSym_address='0x55555556b752')]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_sel_df.collect()[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85749325",
   "metadata": {},
   "source": [
    "#### 列表示方式"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9554c78c",
   "metadata": {},
   "source": [
    "##### 以文本列名表示列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b70febb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------+\n",
      "|          fnSym_name| fnSym_address|\n",
      "+--------------------+--------------+\n",
      "|_ZNSt12_Vector_ba...|0x55555556b4fe|\n",
      "|_ZNSaISt10shared_...|0x55555556b752|\n",
      "+--------------------+--------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.select(\"fnSym_name\",\"fnSym_address\").show(n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5902781c",
   "metadata": {},
   "source": [
    "##### 以pyspark.sql.column.Column表示列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c169738e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.column.Column"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col__fnSym_name=df1[\"fnSym_name\"]\n",
    "type(col__fnSym_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c336c677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------+\n",
      "|          fnSym_name| fnSym_address|\n",
      "+--------------------+--------------+\n",
      "|_ZNSt12_Vector_ba...|0x55555556b4fe|\n",
      "|_ZNSaISt10shared_...|0x55555556b752|\n",
      "+--------------------+--------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.select(df1[\"fnSym_name\"],\"fnSym_address\").show(n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193b7ac7",
   "metadata": {},
   "source": [
    "#### distinct：列唯一值"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b108e0",
   "metadata": {},
   "source": [
    "##### distinct结果类型依然是pyspark.sql.dataframe.DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "eb9d2039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "+----------------+\n",
      "|fnSym_moduleName|\n",
      "+----------------+\n",
      "|   libtorch.so.1|\n",
      "|       libc10.so|\n",
      "|    libcaffe2.so|\n",
      "|   simple_nn.elf|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_selDistn_df=df1.select(\"fnSym_moduleName\").distinct()\n",
    "print(type(_selDistn_df))\n",
    "_selDistn_df.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5a8962",
   "metadata": {},
   "source": [
    "##### 选择列fnSym_fileName的唯一值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "133d2be9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(fnSym_fileName='./aten/src/ATen/native/cpu/avx_mathfun.h'),\n",
       " Row(fnSym_fileName='./aten/src/ATen/Context.h'),\n",
       " Row(fnSym_fileName='/usr/include/c++/9/bits/shared_ptr.h'),\n",
       " Row(fnSym_fileName='./third_party/cpuinfo/src/init.c'),\n",
       " Row(fnSym_fileName='/usr/include/c++/9/bits/stl_algobase.h'),\n",
       " Row(fnSym_fileName='./torch/csrc/autograd/variable.cpp'),\n",
       " Row(fnSym_fileName='/usr/include/c++/9/bits/stl_uninitialized.h'),\n",
       " Row(fnSym_fileName='/usr/include/c++/9/bits/char_traits.h'),\n",
       " Row(fnSym_fileName='./aten/src/ATen/native/TensorShape.cpp'),\n",
       " Row(fnSym_fileName='./c10/core/Storage.h')]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.select(\"fnSym_fileName\").distinct().collect()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879ac68f",
   "metadata": {},
   "source": [
    "#### 列别名"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5912531",
   "metadata": {},
   "source": [
    "##### 选3列，其中一列取别名"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbeb4944",
   "metadata": {},
   "source": [
    "字段direct取别名\"进或出\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f3a411e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------+------+\n",
      "|fnCallId| fnSym_address|进或出|\n",
      "+--------+--------------+------+\n",
      "|       1|0x55555556b4fe|     1|\n",
      "|       2|0x55555556b752|     1|\n",
      "|       3|0x55555556cc0c|     1|\n",
      "|       3|0x55555556cc0c|     2|\n",
      "+--------+--------------+------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df1.select( \"fnCallId\", \"fnSym_address\", df1.direct .alias(\"进或出\")  ) \\\n",
    ".show(n=4)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a417844",
   "metadata": {},
   "source": [
    "#### 以when将枚举列的数值转为文本描述"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ecd319f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------------+\n",
      "|fnCallId|枚举字段direct的文本描述|\n",
      "+--------+------------------------+\n",
      "|       1|                进入函数|\n",
      "|       2|                进入函数|\n",
      "|       3|                进入函数|\n",
      "|       3|                退出函数|\n",
      "|       2|                退出函数|\n",
      "+--------+------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.select(df1[\"fnCallId\"], \\\n",
    "  F.when(df1.direct==1, \"进入函数\") .when(df1.direct==2, \"退出函数\") .otherwise(\"错误值\") .alias(\"枚举字段direct的文本描述\")\n",
    " ).show(n=5)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775afc7e",
   "metadata": {},
   "source": [
    "#### select中写列条件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "dc5de494",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+\n",
      "|(direct = 1)|fnCallId|\n",
      "+------------+--------+\n",
      "|        true|       1|\n",
      "|        true|       2|\n",
      "|        true|       3|\n",
      "|       false|       3|\n",
      "|       false|       2|\n",
      "|        true|       4|\n",
      "|       false|       4|\n",
      "|       false|       1|\n",
      "|        true|       5|\n",
      "|        true|       6|\n",
      "+------------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.select(df1['direct'] == 1, df1.fnCallId).show(n=10)  \n",
    "#该行 是否 为 函数进入 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6bf195",
   "metadata": {},
   "source": [
    "#### isin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bc5e76ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(curThreadId=392463, direct=1, fnAdr='0x7ffff766951f', fnCallId=111, fnSym=Row(address='0x7ffff766951f', column=56, fileName='/home/z/torch-repo/pytorch/torch/csrc/jit/tracer.cpp', lineNumber=143, moduleName='libtorch.so.1', name='getTracingState'), fnSym_address='0x7ffff766951f', fnSym_column=56, fnSym_fileName='./torch/csrc/jit/tracer.cpp', fnSym_lineNumber=143, fnSym_moduleName='libtorch.so.1', fnSym_name='getTracingState'),\n",
       " Row(curThreadId=392463, direct=1, fnAdr='0x7ffff766a41d', fnCallId=112, fnSym=Row(address='0x7ffff766a41d', column=3, fileName='/home/z/torch-repo/pytorch/torch/csrc/jit/tracer.cpp', lineNumber=254, moduleName='libtorch.so.1', name='__tls_init'), fnSym_address='0x7ffff766a41d', fnSym_column=3, fnSym_fileName='./torch/csrc/jit/tracer.cpp', fnSym_lineNumber=254, fnSym_moduleName='libtorch.so.1', fnSym_name='__tls_init'),\n",
       " Row(curThreadId=392463, direct=2, fnAdr='0x7ffff766a41d', fnCallId=112, fnSym=Row(address='0x7ffff766a41d', column=3, fileName='/home/z/torch-repo/pytorch/torch/csrc/jit/tracer.cpp', lineNumber=254, moduleName='libtorch.so.1', name='__tls_init'), fnSym_address='0x7ffff766a41d', fnSym_column=3, fnSym_fileName='./torch/csrc/jit/tracer.cpp', fnSym_lineNumber=254, fnSym_moduleName='libtorch.so.1', fnSym_name='__tls_init')]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1[df1.fnSym_fileName.isin(\\\n",
    "'./torch/csrc/jit/tracer.cpp',\\\n",
    "'./aten/src/ATen/core/Tensor.cpp')].collect() [:3]\n",
    "# 显示符合指定条件的 firstName 列 的记录"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f145c8",
   "metadata": {},
   "source": [
    "#### like "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "dd53b200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------+----------------------------------------+\n",
      "|fnCallId| fnSym_address|fnSym_fileName LIKE ./aten/src/ATen/core|\n",
      "+--------+--------------+----------------------------------------+\n",
      "|       1|0x55555556b4fe|                                   false|\n",
      "|       2|0x55555556b752|                                   false|\n",
      "|       3|0x55555556cc0c|                                   false|\n",
      "|       3|0x55555556cc0c|                                   false|\n",
      "+--------+--------------+----------------------------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.select(\"fnCallId\", \"fnSym_address\",\n",
    "df1.fnSym_fileName.like(\"./aten/src/ATen/core\")  ) \\\n",
    ".show(n=4)   \n",
    "# 显示 fnSym_fileName 列中包含 '/home/z/torch-repo/pytorch/aten/src/ATen/core.cpp\" 的 \"fnCallId\"、 \"fnSym_address\" 列的记录"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584e7fe0",
   "metadata": {},
   "source": [
    "####  startswith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1493fd3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------+------------------------+\n",
      "|fnCallId| fnSym_address|以./aten/...开头的文件名|\n",
      "+--------+--------------+------------------------+\n",
      "|       1|0x55555556b4fe|                   false|\n",
      "|       2|0x55555556b752|                   false|\n",
      "|       3|0x55555556cc0c|                   false|\n",
      "|       3|0x55555556cc0c|                   false|\n",
      "|       2|0x55555556b752|                   false|\n",
      "+--------+--------------+------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.select(\"fnCallId\", \"fnSym_address\",\n",
    "df1.fnSym_fileName.startswith(\"./aten/src/ATen/core\").alias(\"以./aten/...开头的文件名\") ) \\\n",
    ".show(n=5)  \n",
    "#显示 fnSym_fileName 列以 \"./aten/src/ATen/core\" 开头的 \"fnCallId\"、 \"fnSym_address\" 列的记录\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae32371b",
   "metadata": {},
   "source": [
    "#### endswith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bb3b8466",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+\n",
      "|以Tensor.cpp结尾的文件名|\n",
      "+------------------------+\n",
      "|                   false|\n",
      "|                   false|\n",
      "|                   false|\n",
      "|                   false|\n",
      "+------------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.select( df1.fnSym_fileName .endswith(\"Tensor.cpp\").alias(\"以Tensor.cpp结尾的文件名\") ) \\\n",
    ".show(n=4)   \n",
    "# 显示以 \"Tensor.cpp\" 结尾的 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2986a9c",
   "metadata": {},
   "source": [
    "#### substr "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4f43b85b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(文件路径第一个字符='/'),\n",
       " Row(文件路径第一个字符='.'),\n",
       " Row(文件路径第一个字符='.'),\n",
       " Row(文件路径第一个字符='/'),\n",
       " Row(文件路径第一个字符='/')]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.select( df1.fnSym_fileName.substr(1, 1)  .alias(\"文件路径第一个字符\") ) \\\n",
    " .collect()   [15:20]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5591c66",
   "metadata": {},
   "source": [
    "#### between "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "34c4cbaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|给定范围的调用id|\n",
      "+----------------+\n",
      "|            true|\n",
      "|            true|\n",
      "|            true|\n",
      "|            true|\n",
      "|            true|\n",
      "|            true|\n",
      "|            true|\n",
      "|            true|\n",
      "|           false|\n",
      "|           false|\n",
      "+----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.select( df1.fnCallId.between(1, 4).alias(\"给定范围的调用id\") ) \\\n",
    ".show(n=10)   \n",
    "#显示介于22岁至24岁之间的 age 列的记录"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff80510",
   "metadata": {},
   "source": [
    "###  列的 增删改"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fffe26be",
   "metadata": {},
   "source": [
    "#### 修改列名、删除列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "be1732a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0--------------------------------\n",
      " direct           | 1                    \n",
      " fnAdr            | 0x55555556b4fe       \n",
      " 函数调用id       | 1                    \n",
      " fnSym            | [0x55555556b4fe, ... \n",
      " fnSym_column     | 0                    \n",
      " 函数所在文件名   |                      \n",
      " fnSym_lineNumber | 0                    \n",
      " fnSym_moduleName | simple_nn.elf        \n",
      " fnSym_name       | _ZNSt12_Vector_ba... \n",
      "-RECORD 1--------------------------------\n",
      " direct           | 1                    \n",
      " fnAdr            | 0x55555556b752       \n",
      " 函数调用id       | 2                    \n",
      " fnSym            | [0x55555556b752, ... \n",
      " fnSym_column     | 0                    \n",
      " 函数所在文件名   |                      \n",
      " fnSym_lineNumber | 0                    \n",
      " fnSym_moduleName | simple_nn.elf        \n",
      " fnSym_name       | _ZNSaISt10shared_... \n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 \\\n",
    ".withColumnRenamed('fnCallId', '函数调用id') \\\n",
    ".withColumnRenamed('fnSym_fileName', '函数所在文件名') \\\n",
    ".drop(\"fnSym_address\", \"curThreadId\") \\\n",
    ".show(n=2,vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926c6e20",
   "metadata": {},
   "source": [
    "### 分组:groupBy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32e4046",
   "metadata": {},
   "source": [
    "#### 按列direct分组后，统计各组内记录行数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2ab59ba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|direct| count|\n",
      "+------+------+\n",
      "|     1|522502|\n",
      "|     2|522501|\n",
      "+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.groupBy(\"direct\")\\\n",
    "  .count() \\\n",
    "  .show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681a3d66",
   "metadata": {},
   "source": [
    "函数进入次数 比 函数退出次数 多1，此即进、出不平衡，多出的1是哪个函数？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9373e916",
   "metadata": {},
   "source": [
    "#### 找到 进、出 不平衡的函数调用id \n",
    "\n",
    "##### api方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1f867f17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------------+\n",
      "|fnCallId|col__fnCallId_cnt|\n",
      "+--------+-----------------+\n",
      "|  522484|                1|\n",
      "+--------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.groupBy(\"fnCallId\")\\\n",
    "  .agg( count('fnCallId').alias('col__fnCallId_cnt') ) \\\n",
    "  .where(col(\"col__fnCallId_cnt\")==1).show()  # where 可以换成filter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f13ccf2",
   "metadata": {},
   "source": [
    "##### sql方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9f7802b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df1.createTempView(\"v_df1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "66717ad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+\n",
      "|fnCallId|cnt|\n",
      "+--------+---+\n",
      "|  522484|  1|\n",
      "+--------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.createOrReplaceTempView(\"v_df1\")\n",
    "_rs_df = spark.sql(\"SELECT fnCallId,count(*) as cnt  FROM v_df1 group by fnCallId having cnt==1\")\n",
    "_rs_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7c68e8",
   "metadata": {},
   "source": [
    "##### 找 进出不平衡的记录行"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed0d738",
   "metadata": {},
   "source": [
    "###### sql方式找进出不平衡的fnCallId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "584bebb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "<class 'list'>\n",
      "<class 'pyspark.sql.types.Row'>\n",
      "row_ls.len= 1\n",
      "Row(fnCallId=522484)\n",
      "522484 ,fnCallId_notBalanced.type= <class 'int'>\n"
     ]
    }
   ],
   "source": [
    "rs_df = spark.sql(\"SELECT fnCallId   FROM v_df1 group by fnCallId having count(*)==1\")\n",
    "row_ls=rs_df.collect()\n",
    "row0=row_ls[0]\n",
    "fnCallId_notBalanced=row0.fnCallId\n",
    "\n",
    "print(type(rs_df))\n",
    "print(type(row_ls))\n",
    "print(type(row0))\n",
    "print(\"row_ls.len=\",len(row_ls))\n",
    "print(row0)\n",
    "print(fnCallId_notBalanced,   \",fnCallId_notBalanced.type=\",type(fnCallId_notBalanced))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a77332",
   "metadata": {},
   "source": [
    "###### api方式找该fnCallId对应的记录行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d5f86c9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(curThreadId=392463, direct=1, fnAdr='0x7ffff61cda54', fnCallId=522484, fnSym=Row(address='0x7ffff61cda54', column=0, fileName='', lineNumber=0, moduleName='libc10.so', name='_ZNSt10_HashtableIN3c1012TensorTypeIdES1_SaIS1_ENSt8__detail9_IdentityESt8equal_toIS1_ESt4hashIS1_ENS3_18_Mod_range_hashingENS3_20_Default_ranged_hashENS3_20_Prime_rehash_policyENS3_17_Hashtable_traitsILb1ELb1ELb1EEEED1Ev'), fnSym_address='0x7ffff61cda54', fnSym_column=0, fnSym_fileName='', fnSym_lineNumber=0, fnSym_moduleName='libc10.so', fnSym_name='_ZNSt10_HashtableIN3c1012TensorTypeIdES1_SaIS1_ENSt8__detail9_IdentityESt8equal_toIS1_ESt4hashIS1_ENS3_18_Mod_range_hashingENS3_20_Default_ranged_hashENS3_20_Prime_rehash_policyENS3_17_Hashtable_traitsILb1ELb1ELb1EEEED1Ev')]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df1.filter(df1.fnCallId==fnCallId_notBalanced).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b4aa8e",
   "metadata": {},
   "source": [
    "### filter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be450117",
   "metadata": {},
   "source": [
    "\n",
    "####  断言 ， 不 存在 fnAdr != fnSym_address 的行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0f9d4e79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.filter( df1[\"fnAdr\"]!=df1[\"fnSym_address\"] )  .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1def914e",
   "metadata": {},
   "source": [
    "### 排序"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a6d7c3",
   "metadata": {},
   "source": [
    "\n",
    "#### sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4312f86d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(curThreadId=392463, direct=1, fnAdr='0x7ffff7807ad4', fnCallId=451561, fnSym=Row(address='0x7ffff7807ad4', column=7, fileName='/usr/include/c++/9/ext/new_allocator.h', lineNumber=119, moduleName='libtorch.so.1', name='deallocate'), fnSym_address='0x7ffff7807ad4', fnSym_column=7, fnSym_fileName='/usr/include/c++/9/ext/new_allocator.h', fnSym_lineNumber=119, fnSym_moduleName='libtorch.so.1', fnSym_name='deallocate'),\n",
       " Row(curThreadId=392463, direct=2, fnAdr='0x7ffff7807ad4', fnCallId=451561, fnSym=Row(address='0x7ffff7807ad4', column=7, fileName='/usr/include/c++/9/ext/new_allocator.h', lineNumber=119, moduleName='libtorch.so.1', name='deallocate'), fnSym_address='0x7ffff7807ad4', fnSym_column=7, fnSym_fileName='/usr/include/c++/9/ext/new_allocator.h', fnSym_lineNumber=119, fnSym_moduleName='libtorch.so.1', fnSym_name='deallocate')]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.sort( df1.fnSym_address.desc() ).collect()[:2]\n",
    "#按函数地址降序排列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0b0eeefd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(curThreadId=392463, direct=1, fnAdr='0x55555557043e', fnCallId=522502, fnSym=Row(address='0x55555557043e', column=0, fileName='', lineNumber=0, moduleName='simple_nn.elf', name='_ZN9__gnu_cxx13new_allocatorIPNSt8__detail15_Hash_node_baseEED2Ev'), fnSym_address='0x55555557043e', fnSym_column=0, fnSym_fileName='', fnSym_lineNumber=0, fnSym_moduleName='simple_nn.elf', fnSym_name='_ZN9__gnu_cxx13new_allocatorIPNSt8__detail15_Hash_node_baseEED2Ev'),\n",
       " Row(curThreadId=392463, direct=2, fnAdr='0x55555557043e', fnCallId=522502, fnSym=Row(address='0x55555557043e', column=0, fileName='', lineNumber=0, moduleName='simple_nn.elf', name='_ZN9__gnu_cxx13new_allocatorIPNSt8__detail15_Hash_node_baseEED2Ev'), fnSym_address='0x55555557043e', fnSym_column=0, fnSym_fileName='', fnSym_lineNumber=0, fnSym_moduleName='simple_nn.elf', fnSym_name='_ZN9__gnu_cxx13new_allocatorIPNSt8__detail15_Hash_node_baseEED2Ev')]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.sort(\"fnCallId\", ascending=False) .collect()[:2]\n",
    "#按字段fnCallId升序排列"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44bfcc1d",
   "metadata": {},
   "source": [
    "#### orderBy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "88cc1970",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(curThreadId=392463, direct=1, fnAdr='0x55555557043e', fnCallId=522502, fnSym=Row(address='0x55555557043e', column=0, fileName='', lineNumber=0, moduleName='simple_nn.elf', name='_ZN9__gnu_cxx13new_allocatorIPNSt8__detail15_Hash_node_baseEED2Ev'), fnSym_address='0x55555557043e', fnSym_column=0, fnSym_fileName='', fnSym_lineNumber=0, fnSym_moduleName='simple_nn.elf', fnSym_name='_ZN9__gnu_cxx13new_allocatorIPNSt8__detail15_Hash_node_baseEED2Ev'),\n",
       " Row(curThreadId=392463, direct=2, fnAdr='0x55555557043e', fnCallId=522502, fnSym=Row(address='0x55555557043e', column=0, fileName='', lineNumber=0, moduleName='simple_nn.elf', name='_ZN9__gnu_cxx13new_allocatorIPNSt8__detail15_Hash_node_baseEED2Ev'), fnSym_address='0x55555557043e', fnSym_column=0, fnSym_fileName='', fnSym_lineNumber=0, fnSym_moduleName='simple_nn.elf', fnSym_name='_ZN9__gnu_cxx13new_allocatorIPNSt8__detail15_Hash_node_baseEED2Ev'),\n",
       " Row(curThreadId=392463, direct=1, fnAdr='0x55555556f5d6', fnCallId=522501, fnSym=Row(address='0x55555556f5d6', column=0, fileName='', lineNumber=0, moduleName='simple_nn.elf', name='_ZNSaIPNSt8__detail15_Hash_node_baseEED1Ev'), fnSym_address='0x55555556f5d6', fnSym_column=0, fnSym_fileName='', fnSym_lineNumber=0, fnSym_moduleName='simple_nn.elf', fnSym_name='_ZNSaIPNSt8__detail15_Hash_node_baseEED1Ev')]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.orderBy([\"fnCallId\",\"fnSym_address\"],ascending=[0,1])  .collect() [:3]\n",
    "#按字段fnCallId降序、字段fnSym_address升序排列"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812a4f10",
   "metadata": {},
   "source": [
    "###  传统SQL样式\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e895f579",
   "metadata": {},
   "source": [
    "\n",
    "#### 创建视图\n",
    "\n",
    "即 将pyspark.DataFrame看作视图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e55fec7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.createGlobalTempView(\"gtView_df1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "adbc8c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df1.createTempView(\"tView_df1\")\n",
    "df1.createOrReplaceTempView(\"tView_df1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0951e44e",
   "metadata": {},
   "source": [
    "#### 以SQL查询视图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f3ae75f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+--------------+--------+--------------------+--------------+------------+--------------+----------------+----------------+--------------------+\n",
      "|curThreadId|direct|         fnAdr|fnCallId|               fnSym| fnSym_address|fnSym_column|fnSym_fileName|fnSym_lineNumber|fnSym_moduleName|          fnSym_name|\n",
      "+-----------+------+--------------+--------+--------------------+--------------+------------+--------------+----------------+----------------+--------------------+\n",
      "|     392463|     1|0x55555556b4fe|       1|[0x55555556b4fe, ...|0x55555556b4fe|           0|              |               0|   simple_nn.elf|_ZNSt12_Vector_ba...|\n",
      "|     392463|     1|0x55555556b752|       2|[0x55555556b752, ...|0x55555556b752|           0|              |               0|   simple_nn.elf|_ZNSaISt10shared_...|\n",
      "+-----------+------+--------------+--------+--------------------+--------------+------------+--------------+----------------+----------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tmp_df = spark.sql(\"SELECT * FROM tView_df1\")\n",
    "tmp_df.show(n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bffa2206",
   "metadata": {},
   "source": [
    "### pyspark.DataFrame 转为其他结构"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3869d3",
   "metadata": {},
   "source": [
    "#### rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4b07ee8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.RDD'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Row(curThreadId=392463, direct=1, fnAdr='0x55555556b4fe', fnCallId=1, fnSym=Row(address='0x55555556b4fe', column=0, fileName='', lineNumber=0, moduleName='simple_nn.elf', name='_ZNSt12_Vector_baseISt10shared_ptrIN5torch3jit6script4TreeEESaIS5_EE12_Vector_implC1Ev'), fnSym_address='0x55555556b4fe', fnSym_column=0, fnSym_fileName='', fnSym_lineNumber=0, fnSym_moduleName='simple_nn.elf', fnSym_name='_ZNSt12_Vector_baseISt10shared_ptrIN5torch3jit6script4TreeEESaIS5_EE12_Vector_implC1Ev')"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1 = df1.rdd   #将 df 转换为 RDD\n",
    "print(type(rdd1))\n",
    "rdd1.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45817dd6",
   "metadata": {},
   "source": [
    "#### json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e1b7b95f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.RDD'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'{\"curThreadId\":392463,\"direct\":1,\"fnAdr\":\"0x55555556b4fe\",\"fnCallId\":1,\"fnSym\":{\"address\":\"0x55555556b4fe\",\"column\":0,\"fileName\":\"\",\"lineNumber\":0,\"moduleName\":\"simple_nn.elf\",\"name\":\"_ZNSt12_Vector_baseISt10shared_ptrIN5torch3jit6script4TreeEESaIS5_EE12_Vector_implC1Ev\"},\"fnSym_address\":\"0x55555556b4fe\",\"fnSym_column\":0,\"fnSym_fileName\":\"\",\"fnSym_lineNumber\":0,\"fnSym_moduleName\":\"simple_nn.elf\",\"fnSym_name\":\"_ZNSt12_Vector_baseISt10shared_ptrIN5torch3jit6script4TreeEESaIS5_EE12_Vector_implC1Ev\"}'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1Json=df1.toJSON()#将 df 转换为 RDD 字符串\n",
    "print(type(df1Json))\n",
    "df1Json.first()   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69dba147",
   "metadata": {},
   "source": [
    "#### pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85fa2a97",
   "metadata": {},
   "source": [
    "```df1.toPandas```使用到pandas，已安装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "cf7aff70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>curThreadId</th>\n",
       "      <th>direct</th>\n",
       "      <th>fnAdr</th>\n",
       "      <th>fnCallId</th>\n",
       "      <th>fnSym</th>\n",
       "      <th>fnSym_address</th>\n",
       "      <th>fnSym_column</th>\n",
       "      <th>fnSym_fileName</th>\n",
       "      <th>fnSym_lineNumber</th>\n",
       "      <th>fnSym_moduleName</th>\n",
       "      <th>fnSym_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>392463</td>\n",
       "      <td>1</td>\n",
       "      <td>0x55555556b4fe</td>\n",
       "      <td>1</td>\n",
       "      <td>(0x55555556b4fe, 0, , 0, simple_nn.elf, _ZNSt1...</td>\n",
       "      <td>0x55555556b4fe</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>simple_nn.elf</td>\n",
       "      <td>_ZNSt12_Vector_baseISt10shared_ptrIN5torch3jit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>392463</td>\n",
       "      <td>1</td>\n",
       "      <td>0x55555556b752</td>\n",
       "      <td>2</td>\n",
       "      <td>(0x55555556b752, 0, , 0, simple_nn.elf, _ZNSaI...</td>\n",
       "      <td>0x55555556b752</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>simple_nn.elf</td>\n",
       "      <td>_ZNSaISt10shared_ptrIN5torch3jit6script4TreeEE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>392463</td>\n",
       "      <td>1</td>\n",
       "      <td>0x55555556cc0c</td>\n",
       "      <td>3</td>\n",
       "      <td>(0x55555556cc0c, 0, , 0, simple_nn.elf, _ZN9__...</td>\n",
       "      <td>0x55555556cc0c</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>simple_nn.elf</td>\n",
       "      <td>_ZN9__gnu_cxx13new_allocatorISt10shared_ptrIN5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>392463</td>\n",
       "      <td>2</td>\n",
       "      <td>0x55555556cc0c</td>\n",
       "      <td>3</td>\n",
       "      <td>(0x55555556cc0c, 0, , 0, simple_nn.elf, _ZN9__...</td>\n",
       "      <td>0x55555556cc0c</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>simple_nn.elf</td>\n",
       "      <td>_ZN9__gnu_cxx13new_allocatorISt10shared_ptrIN5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>392463</td>\n",
       "      <td>2</td>\n",
       "      <td>0x55555556b752</td>\n",
       "      <td>2</td>\n",
       "      <td>(0x55555556b752, 0, , 0, simple_nn.elf, _ZNSaI...</td>\n",
       "      <td>0x55555556b752</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>simple_nn.elf</td>\n",
       "      <td>_ZNSaISt10shared_ptrIN5torch3jit6script4TreeEE...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   curThreadId  direct           fnAdr  fnCallId  \\\n",
       "0       392463       1  0x55555556b4fe         1   \n",
       "1       392463       1  0x55555556b752         2   \n",
       "2       392463       1  0x55555556cc0c         3   \n",
       "3       392463       2  0x55555556cc0c         3   \n",
       "4       392463       2  0x55555556b752         2   \n",
       "\n",
       "                                               fnSym   fnSym_address  \\\n",
       "0  (0x55555556b4fe, 0, , 0, simple_nn.elf, _ZNSt1...  0x55555556b4fe   \n",
       "1  (0x55555556b752, 0, , 0, simple_nn.elf, _ZNSaI...  0x55555556b752   \n",
       "2  (0x55555556cc0c, 0, , 0, simple_nn.elf, _ZN9__...  0x55555556cc0c   \n",
       "3  (0x55555556cc0c, 0, , 0, simple_nn.elf, _ZN9__...  0x55555556cc0c   \n",
       "4  (0x55555556b752, 0, , 0, simple_nn.elf, _ZNSaI...  0x55555556b752   \n",
       "\n",
       "   fnSym_column fnSym_fileName  fnSym_lineNumber fnSym_moduleName  \\\n",
       "0             0                                0    simple_nn.elf   \n",
       "1             0                                0    simple_nn.elf   \n",
       "2             0                                0    simple_nn.elf   \n",
       "3             0                                0    simple_nn.elf   \n",
       "4             0                                0    simple_nn.elf   \n",
       "\n",
       "                                          fnSym_name  \n",
       "0  _ZNSt12_Vector_baseISt10shared_ptrIN5torch3jit...  \n",
       "1  _ZNSaISt10shared_ptrIN5torch3jit6script4TreeEE...  \n",
       "2  _ZN9__gnu_cxx13new_allocatorISt10shared_ptrIN5...  \n",
       "3  _ZN9__gnu_cxx13new_allocatorISt10shared_ptrIN5...  \n",
       "4  _ZNSaISt10shared_ptrIN5torch3jit6script4TreeEE...  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1_pan=df1.toPandas()   #将 df 的内容转为 Pandas 的数据帧\n",
    "df1_pan.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb3a7ec",
   "metadata": {},
   "source": [
    "### 保存查询结果到文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6001ddad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.readwriter.DataFrameWriter"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_w=df1.select(\"fnCallId\", \"fnSym_address\") .write\n",
    "type(_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4168a84",
   "metadata": {},
   "source": [
    "pyspark.sql.readwriter.DataFrameWriter的save方法的format参数，https://spark.apache.org/docs/3.1.3/api/python/reference/api/pyspark.sql.DataFrameWriter.format.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3629f1a",
   "metadata": {},
   "source": [
    "#### 保存为parquet格式\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a0c8a816",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/26 14:35:46 WARN MemoryManager: Total allocation exceeds 95.00% (957,795,521 bytes) of heap memory\n",
      "Scaling row group sizes to 89.20% for 8 writers\n",
      "24/03/26 14:35:46 WARN MemoryManager: Total allocation exceeds 95.00% (957,795,521 bytes) of heap memory\n",
      "Scaling row group sizes to 79.29% for 9 writers\n",
      "24/03/26 14:35:46 WARN MemoryManager: Total allocation exceeds 95.00% (957,795,521 bytes) of heap memory\n",
      "Scaling row group sizes to 71.36% for 10 writers\n",
      "24/03/26 14:35:46 WARN MemoryManager: Total allocation exceeds 95.00% (957,795,521 bytes) of heap memory\n",
      "Scaling row group sizes to 64.87% for 11 writers\n",
      "24/03/26 14:35:46 WARN MemoryManager: Total allocation exceeds 95.00% (957,795,521 bytes) of heap memory\n",
      "Scaling row group sizes to 59.47% for 12 writers\n",
      "24/03/26 14:35:46 WARN MemoryManager: Total allocation exceeds 95.00% (957,795,521 bytes) of heap memory\n",
      "Scaling row group sizes to 54.89% for 13 writers\n",
      "24/03/26 14:35:46 WARN MemoryManager: Total allocation exceeds 95.00% (957,795,521 bytes) of heap memory\n",
      "Scaling row group sizes to 50.97% for 14 writers\n",
      "24/03/26 14:35:46 WARN MemoryManager: Total allocation exceeds 95.00% (957,795,521 bytes) of heap memory\n",
      "Scaling row group sizes to 47.57% for 15 writers\n",
      "24/03/26 14:35:46 WARN MemoryManager: Total allocation exceeds 95.00% (957,795,521 bytes) of heap memory\n",
      "Scaling row group sizes to 44.60% for 16 writers\n",
      "[Stage 85:>                                                       (0 + 16) / 16]24/03/26 14:35:46 WARN MemoryManager: Total allocation exceeds 95.00% (957,795,521 bytes) of heap memory\n",
      "Scaling row group sizes to 47.57% for 15 writers\n",
      "24/03/26 14:35:46 WARN MemoryManager: Total allocation exceeds 95.00% (957,795,521 bytes) of heap memory\n",
      "Scaling row group sizes to 50.97% for 14 writers\n",
      "24/03/26 14:35:46 WARN MemoryManager: Total allocation exceeds 95.00% (957,795,521 bytes) of heap memory\n",
      "Scaling row group sizes to 54.89% for 13 writers\n",
      "24/03/26 14:35:46 WARN MemoryManager: Total allocation exceeds 95.00% (957,795,521 bytes) of heap memory\n",
      "Scaling row group sizes to 59.47% for 12 writers\n",
      "24/03/26 14:35:46 WARN MemoryManager: Total allocation exceeds 95.00% (957,795,521 bytes) of heap memory\n",
      "Scaling row group sizes to 64.87% for 11 writers\n",
      "24/03/26 14:35:46 WARN MemoryManager: Total allocation exceeds 95.00% (957,795,521 bytes) of heap memory\n",
      "Scaling row group sizes to 71.36% for 10 writers\n",
      "24/03/26 14:35:46 WARN MemoryManager: Total allocation exceeds 95.00% (957,795,521 bytes) of heap memory\n",
      "Scaling row group sizes to 79.29% for 9 writers\n",
      "24/03/26 14:35:46 WARN MemoryManager: Total allocation exceeds 95.00% (957,795,521 bytes) of heap memory\n",
      "Scaling row group sizes to 89.20% for 8 writers\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df1.select(\"fnCallId\", \"fnSym_address\") .write \\\n",
    "  .save(\"fnCallId__fnSym_address.parquet\",format=\"parquet\",mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037bee22",
   "metadata": {},
   "source": [
    "#### 保存为json格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "1afb5f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.select(\"fnCallId\", \"fnSym_address\") .write \\\n",
    "  .save(\"fnCallId__fnSym_address.json\",format=\"json\",mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c443c1",
   "metadata": {},
   "source": [
    "### 终止SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "09accb3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
